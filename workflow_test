#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
###########################################################################
# Copyright © 1998 - 2025 Tencent. All Rights Reserved.
# (部分代码源自腾讯AI Arena，经修改以适配CartPole环境)
#
# 该脚本已整合并修改，以移除外部依赖，并在Gymnasium的CartPole-v1环境中运行。
###########################################################################

import torch
from torch import nn
from torch.distributions import Categorical
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import gymnasium as gym
import time
import os
from collections import namedtuple
import math

# --- 1. 数据结构定义 (原 definition.py 部分) ---
# 使用namedtuple替代kaiwu_agent.utils.common_func中的create_cls
SampleData = namedtuple("SampleData", ["rewards", "done", "last_state", "episode"])
ObsData = namedtuple("ObsData", ["feature", "legal_actions", "done"])
ActData = namedtuple("ActData", ["act"])


# --- 2. 配置 (原 conf.py 部分) ---
# Configuration of dimensions, adapted for CartPole-v1
# 针对CartPole-v1环境修改的维度配置
class Config:
    # --- 环境与模型维度配置 ---
    # CartPole的观测空间是4维 (cart_pos, cart_vel, pole_angle, pole_vel)
    INPUT_SIZE = 4
    # CartPole的动作空间是2维 (向左推, 向右推)
    OUTPUT_SIZE = 2
    # LSTM序列长度
    LSTM_SEQ_LENGTH = 32
    # LSTM隐藏层大小
    LSTM_HIDDEN_SIZE = 64
    # LSTM层数
    LSTM_HIDDEN_LAYERS = 1

    # --- PPO 算法超参数 ---
    GAMMA = 0.99
    EPSILON = 0.5
    K_STEPS = 15

    # --- 学习率与损失函数权重 ---
    LR_ACTOR = 3e-4
    LR_CRITIC = 3e-4
    LR_LSTM = 3e-4
    # 损失函数各项的权重
    LOSS_WEIGHT = {'actor': 0.5, 'critic': 0.5, 'entropy': 0.01}

    # --- 训练流程配置 ---
    EPISODES = 20000  # 总训练回合数
    # CartPole通常在几百步内结束，这里设置一个上限
    MAX_EPISODE_STEPS = 2000
    # 测试回合数
    TEST_EPISODES = 100


# --- 3. 辅助函数 (原 definition.py 部分) ---

@torch.no_grad()  # 该函数不涉及梯度计算
def sample_process(list_game_data, gamma, last_state, episode):
    """
    处理一个回合收集到的样本数据。
    """
    rewards = []
    dones = []
    for sample in list_game_data:
        rewards.append(sample.reward)
        dones.append(sample.done)

    return SampleData(rewards=rewards, done=dones, last_state=last_state, episode=episode)


# --- 4. 模型定义 (原 model.py 部分) ---

def orthogonal_init(layer, gain=1.0):
    """
    对线性层或LSTM层应用正交初始化。
    """
    if isinstance(layer, nn.Linear):
        nn.init.orthogonal_(layer.weight, gain=gain)
        if layer.bias is not None:
            nn.init.constant_(layer.bias, 0)
    elif isinstance(layer, nn.LSTM):
        for name, param in layer.named_parameters():
            if 'weight' in name:
                nn.init.orthogonal_(param, gain=gain)
            elif 'bias' in name:
                nn.init.constant_(param, 0)


class DualLSTM(nn.Module):
    """
    双LSTM网络，但在当前简化逻辑中，主要使用状态LSTM。
    动作序列通过Embedding后与状态特征融合。
    """

    def __init__(self, state_input_dim, num_actions, num_layers=1, hidden_dim=64, action_embedding_dim=8,
                 use_orthogonal_init=True):
        super(DualLSTM, self).__init__()
        self.state_lstm = nn.LSTM(
            input_size=state_input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )
        self.action_embedding = nn.Embedding(num_actions, action_embedding_dim)
        # 移除了action_embedded的cat操作，因此这里fusion_fc1的输入维度也变了
        self.fusion_fc1 = nn.Linear(hidden_dim, hidden_dim)
        self.fusion_fc2 = nn.Linear(hidden_dim, hidden_dim)

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        if use_orthogonal_init:
            orthogonal_init(self.state_lstm)
            orthogonal_init(self.fusion_fc1)
            orthogonal_init(self.fusion_fc2)

    def _init_hidden(self, batch_size, device):
        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device),
                torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device))

    def forward(self, state_seq, action_seq):
        device = state_seq.device
        state_h0, state_c0 = self._init_hidden(state_seq.size(0), device)
        state_out, _ = self.state_lstm(state_seq, (state_h0, state_c0))
        state_out = state_out[:, -1, :]
        # Original code had:
        # last_action_id = action_seq[:, -1, :]
        # last_action_id = torch.argmax(last_action_id, dim=-1)
        # action_embedded = self.action_embedding(last_action_id.long())
        # combined = torch.cat([state_out, action_embedded], dim=1)
        # Now it only uses state_out as per the user's previously provided code
        combined = state_out

        fused_output = torch.tanh(self.fusion_fc1(combined))
        fused_output = torch.tanh(self.fusion_fc2(fused_output))
        return fused_output


class ActorCritic(nn.Module):
    """
    基于Dual LSTM的Actor-Critic网络。
    """

    def __init__(self, input_dim, output_dim, num_layers=1, hidden_dim=64, use_orthogonal_init=True):
        super(ActorCritic, self).__init__()
        self.dual_lstm = DualLSTM(input_dim, output_dim, num_layers, hidden_dim,
                                  use_orthogonal_init=use_orthogonal_init)
        self.actor_output_layer = nn.Linear(hidden_dim, output_dim)
        self.critic_output_layer = nn.Linear(hidden_dim, 1)

        if use_orthogonal_init:
            orthogonal_init(self.actor_output_layer, gain=0.01)
            orthogonal_init(self.critic_output_layer)

    def forward(self, state_seq, action_seq):
        features = self.dual_lstm(state_seq, action_seq)
        action_probs = torch.softmax(self.actor_output_layer(features), dim=-1)
        state_value = self.critic_output_layer(features)
        return action_probs, state_value

    def evaluate(self, state_seq, action_seq, action):
        action_probs, state_value = self.forward(state_seq, action_seq)
        dist = Categorical(action_probs)
        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        return action_logprobs, state_value, dist_entropy

    @torch.no_grad()
    def exploit(self, state_seq, action_seq, legal_actions):
        action_probs, _ = self.forward(state_seq, action_seq)
        mask = torch.zeros_like(action_probs)
        if legal_actions:
            mask[0, legal_actions] = 1.0
        else:  # 如果没有合法动作（理论上在CartPole不发生），允许所有
            mask.fill_(1.0)

        action_probs_masked = action_probs * mask
        action = torch.argmax(action_probs_masked, dim=-1)
        return action


class Model(nn.Module):
    def __init__(self, input_dim, output_dim, seq_length, lr_actor, lr_critic, lr_lstm, eps_clip, K_epochs,
                 loss_weight,
                 lstm_hidden_dim, lstm_num_layers, total_training_steps):
        super().__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Model is using device: {self.device}")

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.seq_length = seq_length
        self.gamma = Config.GAMMA
        self.gae_lambda = 0.95  # GAE lambda
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs
        self.loss_weight = loss_weight

        self.policy = ActorCritic(input_dim, output_dim, lstm_num_layers, lstm_hidden_dim).to(self.device)
        self.optimizer = torch.optim.Adam([
            {'params': self.policy.actor_output_layer.parameters(), 'lr': lr_actor},
            {'params': self.policy.critic_output_layer.parameters(), 'lr': lr_critic},
            {'params': self.policy.dual_lstm.parameters(), 'lr': lr_lstm}
        ])

        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=total_training_steps, eta_min=1e-6)

        self.state_seq = torch.zeros(1, seq_length, input_dim, device=self.device)
        self.action_seq = torch.zeros(1, seq_length, output_dim, device=self.device)
        self.seq_idx = 0

        self._clear_buffer()

    def _clear_buffer(self):
        self.buffer = {
            'state_seqs': [], 'action_seqs': [], 'actions': [], 'logprobs': [],
            'values': [], 'dones': [], 'rewards': []
        }
        self.state_seq.zero_()
        self.action_seq.zero_()
        self.seq_idx = 0

    def reset(self):
        self._clear_buffer()

    def _state_progress(self, state):
        state = torch.FloatTensor(state).to(self.device)
        state = torch.clamp(state, min=-1e4, max=1e4)
        state = torch.nan_to_num(state, nan=0.0, posinf=1e4, neginf=-1e4)

        if self.seq_idx < self.seq_length:
            self.state_seq[0, self.seq_idx, :] = state
        else:
            self.state_seq = torch.roll(self.state_seq, shifts=-1, dims=1)
            self.state_seq[0, -1, :] = state

        if self.seq_idx < self.seq_length:
            self.seq_idx += 1

        return self.state_seq

    def _action_process(self, action):
        one_hot_action = F.one_hot(action, num_classes=self.output_dim).float().unsqueeze(0).to(self.device)
        current_idx = min(self.seq_idx - 1, self.seq_length - 1)

        if self.seq_idx < self.seq_length:
            self.action_seq[0, current_idx, :] = one_hot_action
        else:
            self.action_seq = torch.roll(self.action_seq, shifts=-1, dims=1)
            self.action_seq[0, -1, :] = one_hot_action

        return self.action_seq

    @torch.no_grad()
    def exploit(self, state, legal_actions):
        state_seq = self._state_progress(state)
        # Note: self.action_seq is managed by _action_process after a step is taken.
        # For exploit, we take the current state_seq and the *current* action_seq (which contains past actions)
        action = self.policy.exploit(state_seq, self.action_seq, legal_actions)
        self._action_process(action)  # Update action sequence for next step
        return action

    @torch.no_grad()
    def predict(self, state, done, legal_actions):
        state_seq = self._state_progress(state)
        action_probs, state_value = self.policy(state_seq, self.action_seq)

        mask = torch.zeros_like(action_probs)
        mask[0, legal_actions] = 1.0

        action_probs_masked = action_probs * mask
        prob_sum = torch.sum(action_probs_masked)
        if prob_sum > 0:
            action_probs_masked /= prob_sum
        else:  # Fallback to uniform on legal actions
            action_probs_masked[0, legal_actions] = 1.0 / len(legal_actions)

        dist = Categorical(action_probs_masked)
        action = dist.sample()

        # 使用原始（未屏蔽的）概率分布计算log_prob，以获得正确的梯度
        log_prob = Categorical(action_probs).log_prob(action)

        action_seq = self._action_process(action)

        self.buffer['state_seqs'].append(state_seq.squeeze(0).cpu())
        self.buffer['action_seqs'].append(action_seq.squeeze(0).cpu())
        self.buffer['actions'].append(action.cpu())
        self.buffer['logprobs'].append(log_prob.cpu())
        self.buffer['values'].append(state_value.cpu())
        self.buffer['dones'].append(torch.tensor(done, dtype=torch.float32))

        return action

    def _compute_gae_and_returns(self, rewards, dones, last_value):
        values = torch.cat(self.buffer['values']).squeeze().numpy()
        buffer_size = len(rewards)
        advantages = np.zeros(buffer_size, dtype=np.float32)
        last_gae_lam = 0

        for step in reversed(range(buffer_size)):
            if step == buffer_size - 1:
                next_non_terminal = 1.0 - dones[-1]
                next_values = last_value
            else:
                next_non_terminal = 1.0 - dones[step + 1]
                next_values = values[step + 1]

            delta = rewards[step] + self.gamma * next_values * next_non_terminal - values[step]
            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam
            advantages[step] = last_gae_lam

        returns = advantages + values
        return torch.tensor(advantages, dtype=torch.float32), torch.tensor(returns, dtype=torch.float32)

    def _calc_loss(self, episode, returns, advantages, old_logprobs, new_logprobs, new_values, entropy):
        ratios = torch.exp(new_logprobs - old_logprobs.detach())

        # Fix UserWarning: std(): degrees of freedom is <= 0 by adding a check for data_size >= 2
        # Or, ensure min_minibatch_size in the training loop
        if advantages.numel() > 1 and advantages.std() > 1e-8:  # Add check for sufficient elements
            advantages = (advantages - advantages.mean()) / advantages.std()
        else:  # Handle case of single element or zero variance
            advantages = advantages - advantages.mean()  # Only center if std is problematic

        surr1 = ratios * advantages
        surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
        actor_loss = -torch.min(surr1, surr2).mean()

        # Fix UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])).
        # Ensure new_values and returns have compatible shapes, e.g., (batch_size,)
        critic_loss = F.mse_loss(new_values.squeeze(-1), returns.detach())

        # 熵奖励随着训练退火，鼓励早期探索
        entropy_bonus = self.loss_weight['entropy'] * entropy.mean()

        loss = (self.loss_weight['actor'] * actor_loss +
                self.loss_weight['critic'] * critic_loss -
                entropy_bonus)
        return loss

    def learn(self, sample, last_state, last_done):
        rewards = np.array(sample.rewards, dtype=np.float32)
        dones = np.array(sample.done, dtype=np.float32)

        with torch.no_grad():
            last_state_seq = self._state_progress(last_state)  # this updates self.state_seq
            _, last_value_tensor = self.policy(last_state_seq, self.action_seq)
            last_value = last_value_tensor.cpu().item() * (1 - last_done)

        advantages, returns = self._compute_gae_and_returns(rewards, dones, last_value)
        advantages = advantages.to(self.device)
        returns = returns.to(self.device)

        state_seqs = torch.stack(self.buffer['state_seqs']).to(self.device)
        action_seqs = torch.stack(self.buffer['action_seqs']).to(self.device)
        actions = torch.stack(self.buffer['actions']).to(self.device).squeeze()
        logprobs = torch.stack(self.buffer['logprobs']).to(self.device).squeeze()

        data_size = len(actions)
        minibatch_size = 64

        for _ in range(self.K_epochs):
            indices = np.arange(data_size)
            np.random.shuffle(indices)

            for start_idx in range(0, data_size, minibatch_size):
                end_idx = min(start_idx + minibatch_size, data_size)
                batch_indices = indices[start_idx:end_idx]

                mb_state_seqs = state_seqs[batch_indices]
                mb_action_seqs = action_seqs[batch_indices]
                mb_actions = actions[batch_indices]
                mb_logprobs = logprobs[batch_indices]
                mb_advantages = advantages[batch_indices]
                mb_returns = returns[batch_indices]

                new_logprobs, new_values, entropy = self.policy.evaluate(
                    mb_state_seqs, mb_action_seqs, mb_actions
                )

                loss = self._calc_loss(sample.episode, mb_returns, mb_advantages, mb_logprobs, new_logprobs, new_values,
                                       entropy)

                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=2.0)
                self.optimizer.step()

        self.scheduler.step()
        self._clear_buffer()

    def save_model(self, path, id="1"):
        if not os.path.exists(path):
            os.makedirs(path)
        model_file_path = os.path.join(path, f"model.ckpt-{id}.pt")
        torch.save({
            "policy": self.policy.state_dict(),
            "optimizer": self.optimizer.state_dict(),
            "scheduler": self.scheduler.state_dict()
        }, model_file_path)
        print(f"Model saved to {model_file_path}")

    def load_model(self, path):
        if not os.path.exists(path):
            print(f"Error: Model file not found at {path}")
            return
        checkpoint = torch.load(path, map_location=self.device)
        self.policy.load_state_dict(checkpoint["policy"])
        self.optimizer.load_state_dict(checkpoint["optimizer"])
        self.scheduler.load_state_dict(checkpoint["scheduler"])
        self.reset()  # Reset internal sequence buffers when loading a model
        print(f"Model loaded from {path}")


# --- 5. 智能体定义 (原 agent.py 部分) ---

class Agent:
    def __init__(self, agent_type="player", device=None, logger=None, monitor=None) -> None:
        # device参数被内部模型决定，这里保留签名以兼容
        self.model = Model(
            input_dim=Config.INPUT_SIZE,
            output_dim=Config.OUTPUT_SIZE,
            seq_length=Config.LSTM_SEQ_LENGTH,
            lr_actor=Config.LR_ACTOR,
            lr_critic=Config.LR_CRITIC,
            lr_lstm=Config.LR_LSTM,
            eps_clip=Config.EPSILON,
            K_epochs=Config.K_STEPS,
            loss_weight=Config.LOSS_WEIGHT,
            lstm_hidden_dim=Config.LSTM_HIDDEN_SIZE,
            lstm_num_layers=Config.LSTM_HIDDEN_LAYERS,
            total_training_steps=Config.EPISODES  # Use EPISODES as T_max for scheduler
        )
        self.gamma = Config.GAMMA

    def reset(self):
        """重置智能体内部模型的状态"""
        self.model.reset()

    def predict(self, list_obs_data):
        """用于训练时（带探索）的动作预测"""
        obs_data = list_obs_data[0]
        state = obs_data.feature
        legal_actions = obs_data.legal_actions
        done = obs_data.done
        act_tensor = self.model.predict(state, done, legal_actions)
        return [ActData(act=act_tensor.cpu())]

    def exploit(self, list_obs_data):
        """用于评估时（无探索）的动作选择"""
        obs_data = list_obs_data[0]
        state = obs_data.feature
        legal_actions = obs_data.legal_actions
        action = self.model.exploit(state, legal_actions)
        return action.cpu().item()

    def learn(self, sample_data, last_state):
        """调用模型进行学习"""
        # 从样本数据中获取最后一个时间步的'done'标志
        last_done = sample_data.done[-1] if sample_data.done else False
        self.model.learn(sample_data, last_state, last_done)

    def observation_process(self, done, obs):
        """
        将环境的原始观测值处理成模型需要的格式。
        对于CartPole，观测值已经是特征，无需复杂处理。
        """
        # CartPole的动作始终是[0, 1]
        legal_actions = [0, 1]
        # 特征就是环境的观测值
        feature = obs
        return ObsData(feature=feature, legal_actions=legal_actions, done=done)

    def action_process(self, act_data):
        """将模型的动作输出转换为环境可接受的格式"""
        return act_data.act.item()

    def save_model(self, path="models", id="final"):  # Default ID for saving
        self.model.save_model(path, id)

    def load_model(self, path="models", id="final"):  # Default ID for loading
        model_filename = f"model.ckpt-{id}.pt"
        full_model_path = os.path.join(path, model_filename)
        self.model.load_model(full_model_path)


# --- 6. 训练流程 (原 train.py 部分) ---

class MockLogger:
    """模拟日志记录器，直接打印到控制台"""

    def info(self, msg):
        print(f"[INFO] {msg}")

    def error(self, msg, exc_info=False):  # Added exc_info parameter
        print(f"[ERROR] {msg}")
        if exc_info:
            import traceback
            traceback.print_exc()


class MockMonitor:
    """模拟监控工具"""

    def put_data(self, data):
        # 在实际应用中，这里会将数据发送到监控系统
        # print(f"[Monitor] {data}")
        pass


class CartPoleEnvAdapter:
    """
    Gymnasium环境的适配器，使其接口与原有训练流程兼容。
    """

    def __init__(self, render_mode=None):
        self.env = gym.make("CartPole-v1", render_mode=render_mode)
        self.current_frame_no = 0

    def reset(self, usr_conf=None):
        observation, info = self.env.reset()
        self.current_frame_no = 0
        extra_info = {"result_code": 0, "result_message": "Success"}
        return observation, extra_info

    def step(self, action):
        _obs, reward, terminated, truncated, info = self.env.step(action)
        self.current_frame_no += 1
        _extra_info = {"result_code": 0, "score_info": {"score": reward}}
        return self.current_frame_no, _obs, terminated, truncated, _extra_info

    def render(self):
        self.env.render()

    def close(self):
        self.env.close()


def workflow(envs, agents, logger=None, monitor=None):
    """
    训练主循环。
    """
    try:
        env, agent = envs[0], agents[0]
        monitor_data = {"reward": 0, "terminated": 0}
        last_report_time = time.time()

        logger.info("Start Training...")
        start_t = time.time()

        total_steps_cnt = 0

        for episode in range(Config.EPISODES):
            agent.reset()
            obs, _ = env.reset()
            done = False
            obs_data = agent.observation_process(done, obs)
            sample_buffer = []
            episode_reward = 0
            episode_steps = 0

            for step in range(Config.MAX_EPISODE_STEPS):
                total_steps_cnt += 1
                episode_steps += 1

                # In training, we use predict for exploration
                list_act_data = agent.predict(list_obs_data=[obs_data])
                act = agent.action_process(list_act_data[0])

                _, _obs, terminated, truncated, _extra_info = env.step(act)

                # 在CartPole中，每一步都给1.0的奖励
                reward = _extra_info["score_info"]["score"]
                done = terminated or truncated

                sample = namedtuple("Frame", ["reward", "done"])(reward=reward, done=done)
                sample_buffer.append(sample)

                episode_reward += reward
                obs = _obs
                obs_data = agent.observation_process(done, obs)

                if done:
                    break

            # --- 学习 ---
            sample_data = sample_process(sample_buffer, agent.gamma, obs, episode)
            last_state_feature = agent.observation_process(done, obs).feature
            agent.learn(sample_data, last_state_feature)

            # --- 日志和监控 ---
            if terminated:
                monitor_data["terminated"] += 1

            current_lr = agent.model.scheduler.get_last_lr()[0]
            logger.info(
                f"Episode {episode + 1}/{Config.EPISODES} | "
                f"Steps: {episode_steps} | "
                f"Reward: {episode_reward:.2f} | "
                f"LR: {current_lr:.6f}"
            )

            if time.time() - last_report_time > 60:
                monitor_data["reward"] = episode_reward  # 简单记录最后一个回合的奖励
                if monitor:
                    monitor.put_data({os.getpid(): monitor_data})
                last_report_time = time.time()

        end_t = time.time()
        logger.info(f"Training finished in {end_t - start_t:.2f} seconds.")
        agent.save_model(id="final")  # Save model after training

    except Exception as e:
        logger.error(f"Workflow error: {e}", exc_info=True)
        raise
    finally:
        if 'env' in locals() and env:
            env.close()


# --- 新增的测试流程 ---
def test_workflow(envs, agents, logger=None):
    """
    测试主循环，使用 exploit 模式评估训练好的模型。
    """
    try:
        env, agent = envs[0], agents[0]
        logger.info("Starting evaluation...")

        # Load the trained model
        # Ensure the 'models' directory exists and 'model.ckpt-final.pt' is saved there after training.
        model_path = "models"
        model_id = "final"
        agent.load_model(path=model_path, id=model_id)

        # Switch policy to evaluation mode (e.g., disable dropout, batchnorm if any)
        # Although CartPole model typically doesn't have these, it's good practice.
        agent.model.policy.eval()

        total_rewards = []

        for episode in range(Config.TEST_EPISODES):
            agent.reset()  # Reset agent's internal state (e.g., LSTM hidden states)
            obs, _ = env.reset()
            done = False
            obs_data = agent.observation_process(done, obs)
            episode_reward = 0
            episode_steps = 0

            for step in range(Config.MAX_EPISODE_STEPS):
                episode_steps += 1

                # Use exploit method for evaluation (no exploration, max probability action)
                act = agent.exploit(list_obs_data=[obs_data])

                _, _obs, terminated, truncated, _extra_info = env.step(act)
                reward = _extra_info["score_info"]["score"]
                done = terminated or truncated

                episode_reward += reward
                obs = _obs
                obs_data = agent.observation_process(done, obs)

                # Optional: Render the environment during evaluation
                env.render()
                time.sleep(0.01)  # Small delay for better visualization

                if done:
                    break

            total_rewards.append(episode_reward)
            logger.info(
                f"Test Episode {episode + 1}/{Config.TEST_EPISODES} | "
                f"Steps: {episode_steps} | "
                f"Reward: {episode_reward:.2f}"
            )

        avg_reward = sum(total_rewards) / len(total_rewards)
        logger.info(f"Evaluation finished. Average reward over {Config.TEST_EPISODES} episodes: {avg_reward:.2f}")

    except Exception as e:
        logger.error(f"Test workflow error: {e}", exc_info=True)
        raise
    finally:
        if 'env' in locals() and env:
            env.close()


# --- 7. 主执行入口 ---
if __name__ == "__main__":
    logger = MockLogger()
    monitor = MockMonitor()  # Monitor is usually for training, not evaluation.

    # --- 训练部分 ---
    print("\n--- Starting Training ---")
    train_env = CartPoleEnvAdapter(render_mode="human")  # Can be "human" or None
    train_agent = Agent()
    try:
        workflow([train_env], [train_agent], logger, monitor)
    except Exception as e:
        logger.error(f"Main training execution failed: {e}")
    finally:
        if train_env:
            train_env.close()
            print("Training Environment closed.")

    # --- 测试部分 ---
    # Give it a moment to ensure training env is fully closed before opening new one
    time.sleep(1)
    print("\n--- Starting Evaluation ---")
    # For evaluation, it's common to use render_mode="human" to visualize the learned policy.
    test_env = CartPoleEnvAdapter(render_mode="human")
    test_agent = Agent()  # Create a new agent instance for evaluation
    try:
        test_workflow([test_env], [test_agent], logger)
    except Exception as e:
        logger.error(f"Main evaluation execution failed: {e}")
    finally:
        if test_env:
            test_env.close()
            print("Evaluation Environment closed.")
